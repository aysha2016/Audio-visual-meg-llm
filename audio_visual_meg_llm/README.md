# ğŸ§  Audio-Visual MEG + LLM Interpretation Toolkit

This project simulates human brain responses (MEG signals) when a person sees and hears objects (e.g., seeing a cat and hearing it meow). It uses visual and audio input, simulates MEG-like responses, and generates natural language interpretations using a Large Language Model (LLM, GPT-2).

## ğŸ“¦ Contents

```
audio_visual_meg_llm/
â”œâ”€â”€ 1_image_audio_loader.py
â”œâ”€â”€ 2_meg_simulation.py
â”œâ”€â”€ 3_llm_interpretation.py
â”œâ”€â”€ sample_image.jpg (optional)
â”œâ”€â”€ sample_audio.wav (optional)
â””â”€â”€ README.md
```

## ğŸš€ How to Use

### 1. Install dependencies

```bash
pip install transformers torchaudio opencv-python matplotlib
```

### 2. Run the scripts

```bash
python 1_image_audio_loader.py
python 2_meg_simulation.py
python 3_llm_interpretation.py
```

## ğŸ’¡ Output

- ğŸï¸ Visual input from an image
- ğŸ”Š Audio waveform display
- ğŸ§  Simulated MEG description
- ğŸ’¬ LLM output such as: "I saw and heard a cat."

Replace input files as needed!
